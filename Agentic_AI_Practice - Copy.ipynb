{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f9cfef5",
   "metadata": {},
   "source": [
    "# Lang Chain Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e73ccd6",
   "metadata": {},
   "source": [
    "## Loading the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e31b600f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3359433f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agentic2.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b73c1667",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "## Langsmith Tracking And Tracing\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=os.getenv(\"LANGCHAIN_TRACING_V2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2e5aa7",
   "metadata": {},
   "source": [
    "## Langchain chatopenai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddf63686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x0000029D73ED08D0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000029D7536CF90> root_client=<openai.OpenAI object at 0x0000029D72DBBB90> root_async_client=<openai.AsyncOpenAI object at 0x0000029D74F3A290> model_name='o1-mini' temperature=1.0 model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(model=\"o1-mini\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef74fd32",
   "metadata": {},
   "source": [
    "langchain chatopenai automatically extract openai_api_key from environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57aaecbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='**Agentic AI** refers to artificial intelligence systems designed to act as autonomous agents capable of perceiving their environment, making decisions, and executing actions to achieve specific goals. The term \"agentic\" emphasizes characteristics such as autonomy, proactivity, and the ability to take initiative, distinguishing these AI systems from more passive or reactive forms of artificial intelligence.\\n\\n### Key Characteristics of Agentic AI\\n\\n1. **Autonomy**: Agentic AI systems operate independently without continuous human intervention. They can make decisions based on their programming and learned experiences.\\n\\n2. **Goal-Directed Behavior**: These AI agents are designed to achieve specific objectives. They can set sub-goals, plan actions, and adjust strategies as needed to accomplish their primary goals.\\n\\n3. **Perception and Environment Interaction**: Agentic AI can perceive its environment through sensors or data inputs and interact with it effectively. This interaction enables the AI to gather information, assess situations, and respond appropriately.\\n\\n4. **Learning and Adaptation**: Many agentic AI systems incorporate machine learning algorithms allowing them to learn from experiences, adapt to new situations, and improve their performance over time.\\n\\n5. **Decision-Making Capabilities**: These AI agents can evaluate options, weigh potential outcomes, and choose actions that align best with their objectives.\\n\\n### Examples of Agentic AI\\n\\n- **Autonomous Vehicles**: Self-driving cars are a prime example of agentic AI. They navigate roads, make real-time decisions based on traffic conditions, and respond to dynamic environments to transport passengers safely.\\n\\n- **Robotics**: Industrial robots in manufacturing settings operate autonomously to perform tasks such as assembly, welding, and packaging, adjusting their actions based on real-time feedback.\\n\\n- **Virtual Assistants and Chatbots**: Advanced virtual assistants can manage schedules, initiate tasks, and engage in complex interactions with users, demonstrating a level of agency in handling various requests.\\n\\n- **Intelligent Personal Assistants**: AI systems that manage emails, set reminders, and optimize daily routines by making proactive decisions to enhance user productivity.\\n\\n### Applications of Agentic AI\\n\\n1. **Healthcare**: AI agents can monitor patient vitals, manage treatment plans, and even perform preliminary diagnostics, acting proactively to improve patient outcomes.\\n\\n2. **Finance**: Autonomous trading systems analyze market data in real-time, make investment decisions, and execute trades with minimal human oversight.\\n\\n3. **Smart Homes and IoT**: AI agents manage home environments by controlling lighting, heating, security systems, and appliances based on user preferences and behaviors.\\n\\n4. **Customer Service**: Advanced chatbots and virtual agents handle customer inquiries, resolve issues, and provide personalized support without human intervention.\\n\\n### Ethical and Societal Considerations\\n\\nWhile agentic AI offers numerous benefits, it also raises several ethical and societal concerns:\\n\\n- **Accountability**: Determining responsibility for the actions of autonomous AI agents can be challenging, especially in scenarios where decisions lead to unintended consequences.\\n\\n- **Bias and Fairness**: AI agents can inherit biases present in their training data or algorithms, leading to unfair or discriminatory outcomes.\\n\\n- **Privacy**: Autonomous agents often require access to vast amounts of data, raising concerns about data privacy and security.\\n\\n- **Job Displacement**: Increased automation through agentic AI may lead to displacement of jobs, necessitating strategies for workforce transition and reskilling.\\n\\n- **Control and Autonomy**: Ensuring that AI agents remain aligned with human values and objectives is critical to prevent scenarios where AI actions diverge from intended goals.\\n\\n### Future Directions\\n\\nThe development of agentic AI is advancing toward more sophisticated and capable systems. Future trends include:\\n\\n- **Enhanced Learning Capabilities**: Incorporating more advanced machine learning techniques to improve adaptability and decision-making.\\n\\n- **Better Human-AI Collaboration**: Designing AI agents that can effectively collaborate with humans, augmenting human capabilities rather than replacing them.\\n\\n- **Robust Ethical Frameworks**: Developing comprehensive ethical guidelines and regulations to govern the deployment and behavior of agentic AI systems.\\n\\n- **Explainability and Transparency**: Ensuring that AI agents can provide understandable explanations for their actions to foster trust and accountability.\\n\\n### Conclusion\\n\\nAgentic AI represents a significant advancement in artificial intelligence, enabling systems to act with a degree of autonomy and purposefulness that can transform various industries and aspects of daily life. While the potential benefits are substantial, it is essential to address the accompanying ethical, societal, and technical challenges to ensure that agentic AI contributes positively to society.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 983, 'prompt_tokens': 13, 'total_tokens': 996, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 64, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o1-mini-2024-09-12', 'system_fingerprint': 'fp_3da8b0b088', 'id': 'chatcmpl-BhG5VSBveTcxG47pk9Se36yGPkKfr', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--8b04a864-56c3-48af-9d3c-ccd7811651c2-0' usage_metadata={'input_tokens': 13, 'output_tokens': 983, 'total_tokens': 996, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 64}}\n"
     ]
    }
   ],
   "source": [
    "result=llm.invoke(\"What is Agentic AI\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaa5c11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Agentic AI** refers to artificial intelligence systems designed to act as autonomous agents capable of perceiving their environment, making decisions, and executing actions to achieve specific goals. The term \"agentic\" emphasizes characteristics such as autonomy, proactivity, and the ability to take initiative, distinguishing these AI systems from more passive or reactive forms of artificial intelligence.\n",
      "\n",
      "### Key Characteristics of Agentic AI\n",
      "\n",
      "1. **Autonomy**: Agentic AI systems operate independently without continuous human intervention. They can make decisions based on their programming and learned experiences.\n",
      "\n",
      "2. **Goal-Directed Behavior**: These AI agents are designed to achieve specific objectives. They can set sub-goals, plan actions, and adjust strategies as needed to accomplish their primary goals.\n",
      "\n",
      "3. **Perception and Environment Interaction**: Agentic AI can perceive its environment through sensors or data inputs and interact with it effectively. This interaction enables the AI to gather information, assess situations, and respond appropriately.\n",
      "\n",
      "4. **Learning and Adaptation**: Many agentic AI systems incorporate machine learning algorithms allowing them to learn from experiences, adapt to new situations, and improve their performance over time.\n",
      "\n",
      "5. **Decision-Making Capabilities**: These AI agents can evaluate options, weigh potential outcomes, and choose actions that align best with their objectives.\n",
      "\n",
      "### Examples of Agentic AI\n",
      "\n",
      "- **Autonomous Vehicles**: Self-driving cars are a prime example of agentic AI. They navigate roads, make real-time decisions based on traffic conditions, and respond to dynamic environments to transport passengers safely.\n",
      "\n",
      "- **Robotics**: Industrial robots in manufacturing settings operate autonomously to perform tasks such as assembly, welding, and packaging, adjusting their actions based on real-time feedback.\n",
      "\n",
      "- **Virtual Assistants and Chatbots**: Advanced virtual assistants can manage schedules, initiate tasks, and engage in complex interactions with users, demonstrating a level of agency in handling various requests.\n",
      "\n",
      "- **Intelligent Personal Assistants**: AI systems that manage emails, set reminders, and optimize daily routines by making proactive decisions to enhance user productivity.\n",
      "\n",
      "### Applications of Agentic AI\n",
      "\n",
      "1. **Healthcare**: AI agents can monitor patient vitals, manage treatment plans, and even perform preliminary diagnostics, acting proactively to improve patient outcomes.\n",
      "\n",
      "2. **Finance**: Autonomous trading systems analyze market data in real-time, make investment decisions, and execute trades with minimal human oversight.\n",
      "\n",
      "3. **Smart Homes and IoT**: AI agents manage home environments by controlling lighting, heating, security systems, and appliances based on user preferences and behaviors.\n",
      "\n",
      "4. **Customer Service**: Advanced chatbots and virtual agents handle customer inquiries, resolve issues, and provide personalized support without human intervention.\n",
      "\n",
      "### Ethical and Societal Considerations\n",
      "\n",
      "While agentic AI offers numerous benefits, it also raises several ethical and societal concerns:\n",
      "\n",
      "- **Accountability**: Determining responsibility for the actions of autonomous AI agents can be challenging, especially in scenarios where decisions lead to unintended consequences.\n",
      "\n",
      "- **Bias and Fairness**: AI agents can inherit biases present in their training data or algorithms, leading to unfair or discriminatory outcomes.\n",
      "\n",
      "- **Privacy**: Autonomous agents often require access to vast amounts of data, raising concerns about data privacy and security.\n",
      "\n",
      "- **Job Displacement**: Increased automation through agentic AI may lead to displacement of jobs, necessitating strategies for workforce transition and reskilling.\n",
      "\n",
      "- **Control and Autonomy**: Ensuring that AI agents remain aligned with human values and objectives is critical to prevent scenarios where AI actions diverge from intended goals.\n",
      "\n",
      "### Future Directions\n",
      "\n",
      "The development of agentic AI is advancing toward more sophisticated and capable systems. Future trends include:\n",
      "\n",
      "- **Enhanced Learning Capabilities**: Incorporating more advanced machine learning techniques to improve adaptability and decision-making.\n",
      "\n",
      "- **Better Human-AI Collaboration**: Designing AI agents that can effectively collaborate with humans, augmenting human capabilities rather than replacing them.\n",
      "\n",
      "- **Robust Ethical Frameworks**: Developing comprehensive ethical guidelines and regulations to govern the deployment and behavior of agentic AI systems.\n",
      "\n",
      "- **Explainability and Transparency**: Ensuring that AI agents can provide understandable explanations for their actions to foster trust and accountability.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "Agentic AI represents a significant advancement in artificial intelligence, enabling systems to act with a degree of autonomy and purposefulness that can transform various industries and aspects of daily life. While the potential benefits are substantial, it is essential to address the accompanying ethical, societal, and technical challenges to ensure that agentic AI contributes positively to society.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeac7de",
   "metadata": {},
   "source": [
    "## Lang chain chat groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d01db12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\\n<think>\\nOkay, the user introduced themselves as Sainadh. I should respond politely and maybe ask how I can assist them. Let me keep it friendly and open-ended so they feel comfortable to ask anything they need help with.\\n\\nHmm, should I use an emoji? Maybe a smiley to keep it friendly. Also, make sure the tone is welcoming. I need to make sure the response is concise but inviting. Alright, that should work.\\n</think>\\n\\nHello Sainadh! 😊 How can I assist you today? Feel free to ask anything!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 111, 'prompt_tokens': 17, 'total_tokens': 128, 'completion_time': 0.269894638, 'prompt_time': 0.003155706, 'queue_time': 0.33128514200000003, 'total_time': 0.273050344}, 'model_name': 'qwen-qwq-32b', 'system_fingerprint': 'fp_512a3da6bb', 'finish_reason': 'stop', 'logprobs': None}, id='run--099a3401-fa3e-4327-8783-d0a37d238403-0', usage_metadata={'input_tokens': 17, 'output_tokens': 111, 'total_tokens': 128})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"qwen-qwq-32b\")\n",
    "model.invoke(\"Hi My name is Sainadh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c4de7e",
   "metadata": {},
   "source": [
    "> qwen-qwq-32b\n",
    "\n",
    "Here’s a detailed overview of **QwQ‑32B** — a powerful reasoning-oriented large language model:\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 What is QwQ‑32B?\n",
    "\n",
    "* **Reasoning-specialized model**: Part of Alibaba Cloud's Qwen family, QwQ‑32B is specifically designed for deep analytical reasoning, surpassing general instruction-tuned models on complex, multi-step tasks ([medium.com][1]).\n",
    "* **Size and architecture**:\n",
    "\n",
    "  * \\~32.5 B total parameters (\\~31 B non-embedding)\n",
    "  * 64 Transformer layers\n",
    "  * 40 Q / 8 KV attention heads\n",
    "  * Supports ultra-long context: up to 131,072 tokens ([huggingface.co][2]).\n",
    "\n",
    "---\n",
    "\n",
    "### ⚙️ Capabilities\n",
    "\n",
    "* **Advanced reasoning**: Strong in math, coding, and challenging problem-solving—achieves top-tier results on benchmarks like AIME24, BFCL, LiveBench, competing closely with models up to 20× larger (e.g., DeepSeek‑R1 of 671 B params) ([datacamp.com][3]).\n",
    "* **Tool-use and function calling**: Built to interact with external tools and manage agentic workflows—outperforming models like o1‑mini in the Berkeley Function Calling leaderboard ([groq.com][4]).\n",
    "* **Self-reflective reasoning (“chain-of-thought”)**: Internally uses RL to pause and reflect (“<think>…”) before producing answers, enhancing accuracy despite verbose outputs ([groq.com][4]).\n",
    "\n",
    "---\n",
    "\n",
    "### 🛠️ Training & Access\n",
    "\n",
    "* **Built by Alibaba Cloud's Qwen team**: Based on pre‑training Qwen 2.5‑32B, then RL‑finetuned for reasoning tasks using verifiers for math and code correctness ([qwenlm.github.io][5]).\n",
    "* **Open-source**: Released under the Apache 2.0 license, available via Hugging Face and ModelScope, and accessible through Qwen Chat ([alibabacloud.com][6]).\n",
    "* **Deployment support**: Available on platforms like Groq Cloud and SambaNova, offering high-speed inference (e.g., \\~400 tokens/sec at competitive pricing) ([groq.com][4]).\n",
    "\n",
    "---\n",
    "\n",
    "### 🌐 Who built it?\n",
    "\n",
    "* **Developer**: **Alibaba Cloud** (Qwen team)\n",
    "* **Part of**: The Qwen LLM family—a growing open-source series from Alibaba, with Qwen3 being the latest ([en.wikipedia.org][7], [arxiv.org][8]).\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 Summary Table\n",
    "\n",
    "| Feature            | Details                                                 |\n",
    "| ------------------ | ------------------------------------------------------- |\n",
    "| **Parameters**     | \\~32 B                                                  |\n",
    "| **Architecture**   | Transformer with RoPE, SwiGLU, RMSNorm, QKV bias        |\n",
    "| **Context window** | Up to 131K tokens                                       |\n",
    "| **Strengths**      | Reasoning, math, code, tool use                         |\n",
    "| **Benchmarks**     | Comparable to much larger models (DeepSeek-R1, o1-mini) |\n",
    "| **Access**         | Open-source (Apache 2.0); hosted on multiple platforms  |\n",
    "| **Built by**       | Alibaba Cloud’s Qwen team                               |\n",
    "\n",
    "---\n",
    "\n",
    "In essence, **QwQ‑32B** is a mid-sized, open-source reasoning powerhouse from Alibaba—delivering sophisticated analytical performance while remaining efficient and accessible.\n",
    "\n",
    "* [barrons.com](https://www.barrons.com/articles/alibaba-baba-stock-adr-price-qwen-deepseek-064f5fcc?utm_source=chatgpt.com)\n",
    "* [time.com](https://time.com/7265415/alibaba-model-ai-china-deepseek/?utm_source=chatgpt.com)\n",
    "* [reuters.com](https://www.reuters.com/technology/alibaba-shares-surge-after-it-unveils-reasoning-model-2025-03-06/?utm_source=chatgpt.com)\n",
    "\n",
    "[1]: https://medium.com/%40ferreradaniel/power-of-ai-reasoning-qwen-qwq-32b-revolutionizes-problem-solving-8c775607826f?utm_source=chatgpt.com \"Power of AI Reasoning: Qwen QwQ-32B Revolutionizes Problem ...\"\n",
    "[2]: https://huggingface.co/Qwen/QwQ-32B?utm_source=chatgpt.com \"Qwen/QwQ-32B - Hugging Face\"\n",
    "[3]: https://www.datacamp.com/blog/qwq-32b?utm_source=chatgpt.com \"QwQ-32B: Features, Access, DeepSeek-R1 Comparison & More\"\n",
    "[4]: https://groq.com/a-guide-to-reasoning-with-qwen-qwq-32b/?utm_source=chatgpt.com \"A Guide to Reasoning with Qwen QwQ 32B - Groq is Fast AI Inference\"\n",
    "[5]: https://qwenlm.github.io/blog/qwq-32b/?utm_source=chatgpt.com \"QwQ-32B: Embracing the Power of Reinforcement Learning - Qwen\"\n",
    "[6]: https://www.alibabacloud.com/blog/alibaba-cloud-unveils-qwq-32b-a-compact-reasoning-model-with-cutting-edge-performance_602039?utm_source=chatgpt.com \"Alibaba Cloud Unveils QwQ-32B: A Compact Reasoning Model with ...\"\n",
    "[7]: https://en.wikipedia.org/wiki/Qwen?utm_source=chatgpt.com \"Qwen\"\n",
    "[8]: https://arxiv.org/abs/2505.09388?utm_source=chatgpt.com \"Qwen3 Technical Report\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654e779b",
   "metadata": {},
   "source": [
    "## Langchain chat prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68c53691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Prompt Engineering\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26c6411d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000029D7CF682D0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000029D7CF68E10>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"gemma2-9b-it\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b39c1e",
   "metadata": {},
   "source": [
    "> gemma2-9b-it\n",
    "\n",
    "**Gemma 2 9B‑IT** is the instruction-tuned, text-generation-focused variant of Google’s open-source Gemma 2 model series. Here's a breakdown of what makes it special:\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 What is Gemma 2 9B-IT?\n",
    "\n",
    "* **Text-to-text, decoder‑only LLM**: Based on the same research and architecture as Google's Gemini models and fine-tuned for instruction-following tasks like chat, summarization, code generation, reasoning, and more ([huggingface.co][1]).\n",
    "* **Mid-sized (\\~9 billion parameters)**: Offers a strong balance between performance and computational efficiency — outperforming models like Llama 3 8B in its category ([blog.google][2]).\n",
    "* **Core design improvements**: Utilizes advanced techniques like group-query attention, extra RMSNorm layers, and distillation from larger variants to boost performance and stability ([developers.googleblog.com][3]).\n",
    "* **Context window**: Typically supports around 8K tokens (as part of the Gemma 2 family), although future variants (Gemma 3) expand this ([docsbot.ai][4]).\n",
    "\n",
    "---\n",
    "\n",
    "### ⚙️ Capabilities & Usage\n",
    "\n",
    "* **Instruction-tuned** (“IT”): Specialized to follow user instructions, making it excellent for chatbots, content creation, Q\\&A, and language transformation.\n",
    "* **Versatile applications**: Works well for text generation (stories, essays), summarization, translation, basic code generation and reasoning, and more ([build.nvidia.com][5]).\n",
    "* **Developer-friendly**: Open weights under Google’s Gemma license, usable via Hugging Face, NVIDIA NIM containers, Ollama, OpenRouter, DeepInfra, etc. ([huggingface.co][1]).\n",
    "* **Hardware-friendly**: Efficient enough to run inference on GPUs (e.g., A100/H100 or even home hardware setups) and via on-device frameworks like Gemma.cpp or llama.cpp ([huggingface.co][6]).\n",
    "\n",
    "---\n",
    "\n",
    "### 🌐 Who built it?\n",
    "\n",
    "* **Developed by**: Google (Gemma Team / Google DeepMind), as part of the Gemma 2 model family ([huggingface.co][1]).\n",
    "* **Instruction-tuned variant**: The “IT” suffix indicates fine-tuning on conversational and instructional datasets to optimize behavior in chat and command-following scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 Community Impressions\n",
    "\n",
    "On Reddit (r/LocalLLaMA), users praise its performance and personality:\n",
    "\n",
    "> “This is the best model I have ever run on my 3060 shit‑box… It’s the first model that genuinely felt better than GPT‑3.5.” ([developers.googleblog.com][7], [reddit.com][8])\n",
    "\n",
    "They highlight its friendly tone, multilingual capabilities, and strong suitability for roleplay — albeit with occasional factual inaccuracies ([reddit.com][8]).\n",
    "\n",
    "---\n",
    "\n",
    "### 📋 Quick Overview\n",
    "\n",
    "| Feature            | Details                                                                               |\n",
    "| ------------------ | ------------------------------------------------------------------------------------- |\n",
    "| **Params**         | \\~9 billion                                                                           |\n",
    "| **Architecture**   | Gemma 2 backbone; instruction‑tuned                                                   |\n",
    "| **Context window** | \\~8K tokens                                                                           |\n",
    "| **Strengths**      | Efficient inference, instruction-following, multilingual writing, creative generation |\n",
    "| **Usage**          | Chatbots, summarization, code, general text                                           |\n",
    "| **Access**         | Open-source on Hugging Face, NVIDIA NIM, Ollama, DeepInfra, etc.                      |\n",
    "| **Developer**      | Google / DeepMind Gemma Team                                                          |\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**\n",
    "Gemma 2 9B‑IT is a highly capable, efficient, instruction‑tuned language model from Google. It's optimized for chat and content generation tasks within a practical 9B parameter size, offering open access for developers and strong performance relative to similarly sized models.\n",
    "\n",
    "Feel free to ask if you want usage examples, benchmarks, or deployment pointers!\n",
    "\n",
    "[1]: https://huggingface.co/google/gemma-2-9b-it?utm_source=chatgpt.com \"google/gemma-2-9b-it - Hugging Face\"\n",
    "[2]: https://blog.google/technology/developers/google-gemma-2/?utm_source=chatgpt.com \"Gemma 2 is now available to researchers and developers\"\n",
    "[3]: https://developers.googleblog.com/gemma-explained-new-in-gemma-2?utm_source=chatgpt.com \"Gemma explained: What's new in Gemma 2 - Google Developers Blog\"\n",
    "[4]: https://docsbot.ai/models/gemma-2-9b?utm_source=chatgpt.com \"Google's Gemma 2 9B - AI Model Details - DocsBot AI\"\n",
    "[5]: https://build.nvidia.com/google/gemma-2-9b-it/modelcard?utm_source=chatgpt.com \"gemma-2-9b-it Model by Google | NVIDIA NIM\"\n",
    "[6]: https://huggingface.co/INSAIT-Institute/BgGPT-Gemma-2-9B-IT-v1.0?utm_source=chatgpt.com \"INSAIT-Institute/BgGPT-Gemma-2-9B-IT-v1.0 - Hugging Face\"\n",
    "[7]: https://developers.googleblog.com/en/gemma-explained-overview-gemma-model-family-architectures/?utm_source=chatgpt.com \"Gemma explained: An overview of Gemma model family architectures\"\n",
    "[8]: https://www.reddit.com/r/LocalLLaMA/comments/1drxhlh/gemma_2_9b_appreciation_post/?utm_source=chatgpt.com \"Gemma 2 9b appreciation post : r/LocalLLaMA - Reddit\"\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffefe77e",
   "metadata": {},
   "source": [
    "## chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81c90369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000029D7CF682D0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000029D7CF68E10>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain=prompt|model\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31f1a0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI engineer, I'm familiar with Langsmith! \n",
      "\n",
      "Langsmith is an open-source platform designed to simplify the development and deployment of large language models (LLMs). Think of it as a toolbox specifically built for working with powerful AI like me. \n",
      "\n",
      "Here are some key things to know about Langsmith:\n",
      "\n",
      "**What it does:**\n",
      "\n",
      "* **Fine-tuning:** Langsmith makes it easier to customize pre-trained LLMs for specific tasks. Imagine taking a general-purpose language model and training it to excel at summarizing medical documents, writing different kinds of creative content, or answering questions about a particular subject. \n",
      "* **Evaluation:** It provides tools to assess the performance of your fine-tuned models. This helps you understand how well your model is doing and identify areas for improvement.\n",
      "* **Deployment:** Langsmith streamlines the process of putting your trained models into action. It can help you create APIs or integrate your models into existing applications.\n",
      "\n",
      "**Why it's useful:**\n",
      "\n",
      "* **Accessibility:**  Langsmith lowers the barrier to entry for working with LLMs. It doesn't require extensive machine learning expertise to get started.\n",
      "* **Efficiency:** It automates many of the tedious tasks involved in LLM development, saving you time and effort.\n",
      "* **Collaboration:** As an open-source platform, Langsmith encourages community contributions and knowledge sharing.\n",
      "\n",
      "**Who uses it:**\n",
      "\n",
      "* **Researchers:**  Langsmith can be used to experiment with new LLM architectures and training techniques.\n",
      "* **Developers:** It's a valuable tool for building AI-powered applications.\n",
      "* **Anyone interested in exploring the potential of LLMs:**  Even if you don't have a deep technical background, Langsmith can be a fun and educational way to learn about this exciting field.\n",
      "\n",
      "\n",
      "Let me know if you have any more questions about Langsmith or LLMs in general!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response=chain.invoke({\"input\":\"Can you tell me something about Langsmith\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cd689a",
   "metadata": {},
   "source": [
    "## Langchain Output Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fb5ec8",
   "metadata": {},
   "source": [
    "### StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0c91736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI engineer, I'm happy to tell you about Langsmith!  \n",
      "\n",
      "Langsmith is an open-source tool developed by AssemblyAI that simplifies the process of fine-tuning large language models (LLMs) for specific tasks.  Essentially, it acts as a user-friendly interface and framework for customizing powerful LLMs like GPT-3 and making them more effective for your particular needs. \n",
      "\n",
      "Here's a breakdown of Langsmith's key features and benefits:\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "* **Simplified Fine-Tuning:** Langsmith streamlines the fine-tuning process, making it accessible to a wider range of users, even those without extensive machine learning expertise.\n",
      "* **Pythonic API:** It provides a Python-based API that's intuitive and easy to integrate into existing workflows.\n",
      "* **Data Management:** Langsmith helps you manage your training data effectively, including splitting it, cleaning it, and formatting it for fine-tuning.\n",
      "* **Experiment Tracking:** It allows you to track your fine-tuning experiments, compare different models and parameters, and easily reproduce your best results.\n",
      "* **Model Deployment:** Langsmith aids in deploying your fine-tuned models for real-world applications.\n",
      "\n",
      "**Benefits:**\n",
      "\n",
      "* **Improved Performance:** Fine-tuning LLMs on your specific data can significantly enhance their accuracy and performance for your target tasks.\n",
      "* **Customization:** You can tailor LLMs to your unique domain or use case, achieving better results than using a generic pre-trained model.\n",
      "* **Cost-Effectiveness:** By fine-tuning existing models instead of training new ones from scratch, you can save on computational resources and time.\n",
      "* **Accessibility:** Langsmith lowers the barrier to entry for using LLMs, enabling more people to leverage their power.\n",
      "\n",
      "**Use Cases:**\n",
      "\n",
      "Langsmith is versatile and can be used for various applications, including:\n",
      "\n",
      "* **Chatbots:** Fine-tune LLMs to create more conversational and context-aware chatbots.\n",
      "* **Text Summarization:** Improve the accuracy and quality of text summarization tasks.\n",
      "* **Code Generation:** Train LLMs to generate code in specific programming languages.\n",
      "* **Question Answering:** Enhance the performance of question-answering systems.\n",
      "* **Sentiment Analysis:** Fine-tune LLMs for more accurate sentiment analysis in text.\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have any more questions about Langsmith or want to delve deeper into specific aspects!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser=StrOutputParser()\n",
    "\n",
    "chain=prompt|model|output_parser\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f184ba5",
   "metadata": {},
   "source": [
    "### JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4659204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return a JSON object.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f28edc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"Answer the user query \\n {format_instruction}\\n {query}\\n \",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instruction\":output_parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24008f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instruction': 'Return a JSON object.'}, template='Answer the user query \\n {format_instruction}\\n {query}\\n ')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42281dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Langsmith', 'description': 'Langsmith is an open-source platform for building and deploying large language models (LLMs). It provides a modular and scalable architecture that allows developers to easily customize and experiment with different LLM components.', 'key_features': ['Modular and extensible design', 'Support for various LLM architectures', 'Easy deployment and management', 'Open-source and community-driven', 'Integration with popular machine learning frameworks'], 'website': 'https://github.com/langs-platform/langs'}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model|output_parser\n",
    "response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55498157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Langsmith is an open-source platform for building and deploying large '\n",
      " 'language models (LLMs). It provides a modular and scalable architecture that '\n",
      " 'allows developers to easily customize and experiment with different LLM '\n",
      " 'components.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(response['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "377712a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer.Provide the response in json.Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer.Provide the response in json.Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7611d5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'Langsmith is an open-source framework developed by the Hugging Face community for fine-tuning large language models (LLMs). \\n\\n  **Key Features:**\\n\\n  * **Simplified Fine-Tuning:** Langsmith makes it easier to fine-tune LLMs by providing a user-friendly interface and streamlined workflows.\\n\\n  * **Model Management:** It offers tools for managing and versioning your fine-tuned models.\\n\\n  * **Dataset Integration:** Langsmith supports various data formats and integrates with popular datasets.\\n\\n  * **Experiment Tracking:** It allows you to track your fine-tuning experiments and compare results.\\n\\n  * **Community-Driven:** As an open-source project, Langsmith benefits from the contributions and support of a large community of developers.\\n\\n  **Benefits:**\\n\\n  * **Accessibility:** Makes fine-tuning LLMs accessible to a wider range of users.\\n  * **Efficiency:** Streamlines the fine-tuning process, saving time and resources.\\n  * **Customization:** Allows you to tailor LLMs to specific tasks and domains.\\n  * **Collaboration:** Fosters collaboration and knowledge sharing within the AI community.\\n\\n  **Use Cases:**\\n\\n  * **Text Generation:** Fine-tune LLMs for tasks like creative writing, summarization, and dialogue systems.\\n  * **Code Generation:** Train LLMs to generate code in different programming languages.\\n  * **Question Answering:**\\n\\n  Adapt LLMs to answer questions based on specific knowledge bases.\\n  * **Sentiment Analysis:** Fine-tune LLMs to classify text sentiment.\\n\\n  **Getting Started:**\\n\\n  Langsmith can be installed using pip. The Hugging Face website provides comprehensive documentation and tutorials to help you get started.'}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model|output_parser\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47cfd824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'Langsmith is an open-source framework developed by the Hugging '\n",
      "             'Face community for fine-tuning large language models (LLMs). \\n'\n",
      "             '\\n'\n",
      "             '  **Key Features:**\\n'\n",
      "             '\\n'\n",
      "             '  * **Simplified Fine-Tuning:** Langsmith makes it easier to '\n",
      "             'fine-tune LLMs by providing a user-friendly interface and '\n",
      "             'streamlined workflows.\\n'\n",
      "             '\\n'\n",
      "             '  * **Model Management:** It offers tools for managing and '\n",
      "             'versioning your fine-tuned models.\\n'\n",
      "             '\\n'\n",
      "             '  * **Dataset Integration:** Langsmith supports various data '\n",
      "             'formats and integrates with popular datasets.\\n'\n",
      "             '\\n'\n",
      "             '  * **Experiment Tracking:** It allows you to track your '\n",
      "             'fine-tuning experiments and compare results.\\n'\n",
      "             '\\n'\n",
      "             '  * **Community-Driven:** As an open-source project, Langsmith '\n",
      "             'benefits from the contributions and support of a large community '\n",
      "             'of developers.\\n'\n",
      "             '\\n'\n",
      "             '  **Benefits:**\\n'\n",
      "             '\\n'\n",
      "             '  * **Accessibility:** Makes fine-tuning LLMs accessible to a '\n",
      "             'wider range of users.\\n'\n",
      "             '  * **Efficiency:** Streamlines the fine-tuning process, saving '\n",
      "             'time and resources.\\n'\n",
      "             '  * **Customization:** Allows you to tailor LLMs to specific '\n",
      "             'tasks and domains.\\n'\n",
      "             '  * **Collaboration:** Fosters collaboration and knowledge '\n",
      "             'sharing within the AI community.\\n'\n",
      "             '\\n'\n",
      "             '  **Use Cases:**\\n'\n",
      "             '\\n'\n",
      "             '  * **Text Generation:** Fine-tune LLMs for tasks like creative '\n",
      "             'writing, summarization, and dialogue systems.\\n'\n",
      "             '  * **Code Generation:** Train LLMs to generate code in '\n",
      "             'different programming languages.\\n'\n",
      "             '  * **Question Answering:**\\n'\n",
      "             '\\n'\n",
      "             '  Adapt LLMs to answer questions based on specific knowledge '\n",
      "             'bases.\\n'\n",
      "             '  * **Sentiment Analysis:** Fine-tune LLMs to classify text '\n",
      "             'sentiment.\\n'\n",
      "             '\\n'\n",
      "             '  **Getting Started:**\\n'\n",
      "             '\\n'\n",
      "             '  Langsmith can be installed using pip. The Hugging Face website '\n",
      "             'provides comprehensive documentation and tutorials to help you '\n",
      "             'get started.'}\n"
     ]
    }
   ],
   "source": [
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144be5ba",
   "metadata": {},
   "source": [
    "### XML output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c770ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer.<response><answer>Your answer here</answer></response>.Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "output_parser=XMLOutputParser()\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer.<response><answer>Your answer here</answer></response>.Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ff59004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instruction': 'The output should be formatted as a XML file.\\n1. Output should conform to the tags below.\\n2. If tags are not given, make them on your own.\\n3. Remember to always open and close all the tags.\\n\\nAs an example, for the tags [\"foo\", \"bar\", \"baz\"]:\\n1. String \"<foo>\\n   <bar>\\n      <baz></baz>\\n   </bar>\\n</foo>\" is a well-formatted instance of the schema.\\n2. String \"<foo>\\n   <bar>\\n   </foo>\" is a badly-formatted instance.\\n3. String \"<foo>\\n   <tag>\\n   </tag>\\n</foo>\" is a badly-formatted instance.\\n\\nHere are the output tags:\\n```\\nNone\\n```'}, template='Answer the user query \\n {format_instruction}\\n {query}\\n ')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser=XMLOutputParser()\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"Answer the user query \\n {format_instruction}\\n {query}\\n \",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instruction\":output_parser.get_format_instructions()},\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "88d60a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='```xml\\n<response>\\n  <info>\\n    <name>Langsmith</name>\\n    <description>Langsmith is an open-weights platform for developing and deploying AI assistants.</description>\\n    <creator>It is created by the Gemma team at Google DeepMind.</creator>\\n    <features>\\n      <feature>Modular design allows for easy customization and extension.</feature>\\n      <feature>Focuses on responsible AI development with transparency and ethical considerations.</feature>\\n      <feature>Supports various AI models and allows users to fine-tune them for specific tasks.</feature>\\n    </features>\\n  </info>\\n</response>\\n```\\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 144, 'prompt_tokens': 195, 'total_tokens': 339, 'completion_time': 0.261818182, 'prompt_time': 0.007734238, 'queue_time': 0.16343249199999998, 'total_time': 0.26955242}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run--e4f61e62-2582-4ee9-91b7-b4fed69358f6-0' usage_metadata={'input_tokens': 195, 'output_tokens': 144, 'total_tokens': 339}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model\n",
    "response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "34856aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('```xml\\n'\n",
      " '<response>\\n'\n",
      " '  <info>\\n'\n",
      " '    <name>Langsmith</name>\\n'\n",
      " '    <description>Langsmith is an open-weights platform for developing and '\n",
      " 'deploying AI assistants.</description>\\n'\n",
      " '    <creator>It is created by the Gemma team at Google DeepMind.</creator>\\n'\n",
      " '    <features>\\n'\n",
      " '      <feature>Modular design allows for easy customization and '\n",
      " 'extension.</feature>\\n'\n",
      " '      <feature>Focuses on responsible AI development with transparency and '\n",
      " 'ethical considerations.</feature>\\n'\n",
      " '      <feature>Supports various AI models and allows users to fine-tune them '\n",
      " 'for specific tasks.</feature>\\n'\n",
      " '    </features>\\n'\n",
      " '  </info>\\n'\n",
      " '</response>\\n'\n",
      " '```\\n')\n"
     ]
    }
   ],
   "source": [
    "pprint(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5776587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='<response><answer>LangChain is an open-source framework designed to simplify the development of applications powered by large language models (LLMs). It provides a suite of tools and components that enable developers to build, chain, and manage LLM interactions in a modular and efficient way.</answer></response>\\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 64, 'prompt_tokens': 39, 'total_tokens': 103, 'completion_time': 0.116363636, 'prompt_time': 0.003605134, 'queue_time': 0.258729756, 'total_time': 0.11996877}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run--d8d73875-78f3-44c7-9ad1-59a5c10ccf21-0' usage_metadata={'input_tokens': 39, 'output_tokens': 64, 'total_tokens': 103}\n"
     ]
    }
   ],
   "source": [
    "##output parser\n",
    "#from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain.output_parsers.xml import XMLOutputParser\n",
    "\n",
    "# XML Output Parser\n",
    "output_parser = XMLOutputParser()\n",
    "\n",
    "# Prompt that instructs the model to return XML\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Respond in this XML format: <response><answer>Your answer here</answer></response>\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Build the chain\n",
    "chain = prompt | model\n",
    "\n",
    "# Run the chain\n",
    "#response = chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "raw_output =chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "# Print result\n",
    "print(raw_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b498c3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why couldn't the bicycle find its way home?\", 'punchline': 'Because it lost its bearings!'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## With Pydantic\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "model = ChatOpenAI(temperature=0.7)\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e2fe2a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': \"Why couldn't the bicycle stand up by itself? Because it was two tired!\"}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Without Pydantic\n",
    "joke_query = \"Tell me a joke .\"\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32480023",
   "metadata": {},
   "source": [
    "pydantic is helpful in JSON output parser to get required format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "19ba37c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<movie>Big</movie>\n",
      "<movie>Forrest Gump</movie>\n",
      "<movie>Saving Private Ryan</movie>\n",
      "<movie>Cast Away</movie>\n",
      "<movie>The Green Mile</movie>\n",
      "<movie>Toy Story (voice of Woody)</movie>\n",
      "<movie>Apollo 13</movie>\n",
      "<movie>Philadelphia</movie>\n",
      "<movie>Sully</movie>\n",
      "<movie>Bridge of Spies</movie>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "actor_query = \"Generate the shortened filmography for Tom Hanks.\"\n",
    "\n",
    "output = model.invoke(\n",
    "    f\"\"\"{actor_query}\n",
    "Please enclose the movies in <movie></movie> tags\"\"\"\n",
    ")\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "87b25b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup=\"Why couldn't the bicycle stand up by itself?\", punchline='Because it was two tired!')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import YamlOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "model = ChatOpenAI(temperature=0.5)\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = YamlOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bfb688",
   "metadata": {},
   "source": [
    "## Assigments: https://python.langchain.com/docs/how_to/#prompt-templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a50b16",
   "metadata": {},
   "source": [
    "Here's a **comprehensive summary** of **all types of prompt templates in LangChain**, covering:\n",
    "\n",
    "* 📌 Classes from your **uploaded image**\n",
    "* 📌 Examples and use-cases\n",
    "* 📌 Concepts from the linked guides:\n",
    "\n",
    "  * Few-shot examples\n",
    "  * Chat format examples\n",
    "  * Partial prompts\n",
    "  * Prompt composition\n",
    "  * Multimodal prompts\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧠 1. **BasePromptTemplate**\n",
    "\n",
    "* **Abstract base** class for all prompt templates.\n",
    "* Others like `PromptTemplate`, `ChatPromptTemplate` inherit from this.\n",
    "\n",
    "---\n",
    "\n",
    "#### 📄 2. **PromptTemplate**\n",
    "\n",
    "* For **simple string-based** prompts.\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "print(template.format(topic=\"dogs\"))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧵 3. **StringPromptTemplate**\n",
    "\n",
    "* Same as `PromptTemplate`, but explicitly focuses on templates with `.format()` method exposure.\n",
    "\n",
    "---\n",
    "\n",
    "#### 💬 4. **ChatPromptTemplate**\n",
    "\n",
    "* Structured prompt for chat models.\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"What is the capital of {country}?\")\n",
    "])\n",
    "chat_prompt.format(country=\"France\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 👤 5. **HumanMessagePromptTemplate**\n",
    "\n",
    "* Message from the **user** in chat.\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate\n",
    "HumanMessagePromptTemplate.from_template(\"Translate {text} to Hindi.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 🤖 6. **AIMessagePromptTemplate**\n",
    "\n",
    "* Message from the **assistant/AI** in a chat.\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import AIMessagePromptTemplate\n",
    "AIMessagePromptTemplate.from_template(\"Sure! Here is the translation.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 🛠 7. **SystemMessagePromptTemplate**\n",
    "\n",
    "* System-level instruction in a chat.\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import SystemMessagePromptTemplate\n",
    "SystemMessagePromptTemplate.from_template(\"You are a scientific research assistant.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔁 8. **ChatMessagePromptTemplate**\n",
    "\n",
    "* Generic chat message with role (human, AI, system, function).\n",
    "\n",
    "```python\n",
    "ChatMessagePromptTemplate.from_template(role=\"function\", template=\"Function name: {name}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧱 9. **BaseChatPromptTemplate, BaseMessagePromptTemplate, BaseStringMessagePromptTemplate**\n",
    "\n",
    "* Abstract base classes. Used internally for inheritance and consistency across chat/message templates.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔄 10. **MessagesPlaceholder**\n",
    "\n",
    "* Used to inject a **list of existing messages** into a prompt dynamically.\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "MessagesPlaceholder(variable_name=\"chat_history\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ✨ 11. **FewShotPromptTemplate**\n",
    "\n",
    "* For **few-shot learning** in **string-based** prompts.\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "example_prompt = PromptTemplate.from_template(\"Q: {question}\\nA: {answer}\")\n",
    "examples = [{\"question\": \"2+2\", \"answer\": \"4\"}, {\"question\": \"3+5\", \"answer\": \"8\"}]\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Q: {input}\\nA:\",\n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "print(prompt.format(input=\"6+7\"))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 💬✨ 12. **FewShotChatMessagePromptTemplate**\n",
    "\n",
    "* Few-shot for **chat-based prompts**.\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate, ChatPromptTemplate\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{output}\")\n",
    "])\n",
    "examples = [{\"input\": \"Hi\", \"output\": \"Hello!\"}]\n",
    "\n",
    "fewshot = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples\n",
    ")\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Be friendly.\"),\n",
    "    fewshot,\n",
    "    (\"human\", \"How are you?\")\n",
    "])\n",
    "print(final_prompt.format(input=\"How are you?\"))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧩 13. **FewShotPromptWithTemplates**\n",
    "\n",
    "* A variant combining **few-shot logic** with multiple templates dynamically.\n",
    "* Used when templates are complex or structured.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🎥 14. **ImagePromptTemplate**\n",
    "\n",
    "* For **multimodal models** (image + text).\n",
    "\n",
    "```python\n",
    "ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Describe the image\"),\n",
    "    (\"user\", [{\"type\": \"image_url\", \"url\": \"{image_url}\"}])\n",
    "])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔗 15. **PipelinePromptTemplate**\n",
    "\n",
    "* Combines multiple prompt templates in a **pipeline**.\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import PipelinePromptTemplate\n",
    "PipelinePromptTemplate(\n",
    "    final_prompt=PromptTemplate.from_template(\"Q: {question}\\nA:\"),\n",
    "    pipeline_prompts=[\n",
    "        (\"question\", PromptTemplate.from_template(\"Summarize: {text}\"))\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧱 16. **StructuredPrompt**\n",
    "\n",
    "* Represents structured formats, often used with output parsers or schemas.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 Partial Prompts – \\[Docs Summary]\n",
    "\n",
    "* Fill some variables now, others later.\n",
    "* Great for **dynamic chaining** and **reuse**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 Prompt Composition – \\[Docs Summary]\n",
    "\n",
    "* Combine strings or message templates via `+` operator or `extend()`.\n",
    "* Encourages **modular prompt building**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🖼 Multimodal Prompts – \\[Docs Summary]\n",
    "\n",
    "* Allow embedding **images, audio, PDFs** inside prompts.\n",
    "* Works only with **multimodal models** like GPT-4V or Gemini.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Summary Table\n",
    "\n",
    "| Template                           | Input Style       | Best For                         |\n",
    "| ---------------------------------- | ----------------- | -------------------------------- |\n",
    "| `PromptTemplate`                   | String            | Simple instructions              |\n",
    "| `ChatPromptTemplate`               | Messages          | Chatbots or assistants           |\n",
    "| `FewShotPromptTemplate`            | String + Examples | Improve learning via demos       |\n",
    "| `FewShotChatMessagePromptTemplate` | Chat + Examples   | Chat-style examples              |\n",
    "| `MessagesPlaceholder`              | List Injection    | Adding existing message history  |\n",
    "| `ImagePromptTemplate`              | Media block       | Multimodal inputs (images/audio) |\n",
    "| `PipelinePromptTemplate`           | Modular pipeline  | Building chained prompts         |\n",
    "| `PartialPrompt`                    | Partial values    | Dynamic reuse                    |\n",
    "| `StructuredPrompt`                 | Structured output | Controlled formatting            |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like a visual diagram (e.g. mindmap or table image) showing relationships and use cases for each class?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e723f999",
   "metadata": {},
   "source": [
    "## Different types of Output parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6359ea",
   "metadata": {},
   "source": [
    "Here’s a comprehensive overview of **all built‑in output parser types in LangChain**, with clear explanations, examples, and when to use each:\n",
    "\n",
    "---\n",
    "\n",
    "### 📦 Output Parser Types\n",
    "\n",
    "([python.langchain.com][1])\n",
    "\n",
    "#### 1. **StringOutputParser**\n",
    "\n",
    "* **Purpose**: Normalizes LLM or Chat output into a plain Python string.\n",
    "* **Example**:\n",
    "\n",
    "  ```python\n",
    "  from langchain_core.output_parsers import StringOutputParser\n",
    "  parser = StringOutputParser()\n",
    "  parser.parse(\"Hello, world!\")  # => \"Hello, world!\"\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **JSON (JsonOutputParser or PydanticParser)**\n",
    "\n",
    "* **Purpose**: Extracts structured JSON using optional Pydantic validation.\n",
    "* **Example**:\n",
    "\n",
    "  ````python\n",
    "  from langchain_core.output_parsers import JsonOutputParser\n",
    "  from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "  class Joke(BaseModel):\n",
    "      setup: str\n",
    "      punchline: str\n",
    "\n",
    "  parser = JsonOutputParser(pydantic_object=Joke)\n",
    "  instructions = parser.get_format_instructions()\n",
    "  # Include `instructions` in prompt...\n",
    "  result = parser.parse(response_text)\n",
    "  # => {'setup': \"...\", 'punchline': \"...\"}\n",
    "  ``` :contentReference[oaicite:4]{index=4}\n",
    "  ````\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **YAML OutputParser**\n",
    "\n",
    "* **Purpose**: Outputs validated structured data in YAML format using Pydantic.\n",
    "* Use `JsonOutputParser(..., mode=\"yaml\")` or `YAMLOutputParser`.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **XML Parser**\n",
    "\n",
    "* **Purpose**: Parses XML-formatted strings into Python `dict`.\n",
    "* Use when dealing with XML-based tool formats. ([reddit.com][2], [python.langchain.com][1])\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **CSV Parser (CommaSeparatedListOutputParser)**\n",
    "\n",
    "* **Purpose**: Converts comma-separated LLM output into a `List[str]`.\n",
    "* **Example**:\n",
    "\n",
    "  ````python\n",
    "  from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "  parser = CommaSeparatedListOutputParser()\n",
    "  instructions = parser.get_format_instructions()\n",
    "  # Prompt: \"List fruits: …{instructions}\"\n",
    "  result = parser.parse(\"Apple, Banana, Cherry\")\n",
    "  # => [\"Apple\", \"Banana\", \"Cherry\"]\n",
    "  ``` :contentReference[oaicite:11]{index=11}\n",
    "  ````\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. **EnumOutputParser**\n",
    "\n",
    "* **Purpose**: Ensures output matches one value from a given Python `Enum`.\n",
    "* Use when only specific categorical responses are valid. ([python.langchain.com][3])\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. **DatetimeOutputParser**\n",
    "\n",
    "* **Purpose**: Parses output into a Python `datetime.datetime`.\n",
    "* **Example**:\n",
    "\n",
    "  ````python\n",
    "  from langchain.output_parsers import DatetimeOutputParser\n",
    "  parser = DatetimeOutputParser()\n",
    "  # Use parser.get_format_instructions() in prompt\n",
    "  parser.parse(\"1991-02-20 00:00:00\")\n",
    "  # => datetime.datetime(1991, 2, 20)\n",
    "  ``` :contentReference[oaicite:16]{index=16}\n",
    "  ````\n",
    "\n",
    "---\n",
    "\n",
    "#### 8. **PandasDataFrame Parser**\n",
    "\n",
    "* **Purpose**: Outputs structured tabular data as a pandas DataFrame.\n",
    "* Great for spreadsheet-style outputs. ([python.langchain.com][4], [python.langchain.com][1])\n",
    "\n",
    "---\n",
    "\n",
    "#### 9. **StructuredOutputParser**\n",
    "\n",
    "* **Purpose**: Generates `Dict[str, str]` with predefined keys only—lighter weight than JSON/YAML.\n",
    "* **Example**:\n",
    "\n",
    "  ````python\n",
    "  from langchain.output_parsers import StructuredOutputParser\n",
    "  parser = StructuredOutputParser.from_names_and_descriptions({\n",
    "      \"meaning\": \"detailed meaning of the expression\"\n",
    "  })\n",
    "  instructions = parser.get_format_instructions()\n",
    "  # Include in chat prompt...\n",
    "  parser.parse(response_text)\n",
    "  # => {\"meaning\": \"...\"}\n",
    "  ``` :contentReference[oaicite:21]{index=21}\n",
    "  ````\n",
    "\n",
    "---\n",
    "\n",
    "#### 10. **OutputFixingParser & RetryWithErrorParser**\n",
    "\n",
    "* **Purpose**: Wrap another parser; on failure, invoke the LLM to:\n",
    "\n",
    "  * *OutputFixingParser*: fix just the failed output.\n",
    "  * *RetryWithErrorParser*: retry with full context.\n",
    "* Useful for making parsing more robust. ([python.langchain.com][1])\n",
    "\n",
    "---\n",
    "\n",
    "#### 11. **OpenAIFunctions & OpenAITools Parsers**\n",
    "\n",
    "* **Purpose**: Use native OpenAI (or tools) function-calling protocol to get structured outputs.\n",
    "* **Example**:\n",
    "\n",
    "  ```python\n",
    "  from langchain.output_parsers import OpenAIFunctionsOutputParser\n",
    "  parser = OpenAIFunctionsOutputParser(functions=...)\n",
    "  # Use this with ChatOpenAI(..., functions=...)\n",
    "  ```\n",
    "* Recommended when using GPT models supporting function calls. ([python.langchain.com][4])\n",
    "\n",
    "---\n",
    "\n",
    "### 🧰 Custom Parsers\n",
    "\n",
    "* **BaseOutputParser** or **BaseGenerationOutputParser**: for complex or bespoke formats.\n",
    "* **Example**: Boolean parser converting `\"YES\"/\"NO\"` to `bool` ([python.langchain.com][3])\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Summary Table\n",
    "\n",
    "| Parser Type                | Output Type              | Use Case                                |\n",
    "| -------------------------- | ------------------------ | --------------------------------------- |\n",
    "| StringOutputParser         | `str`                    | Normalize raw outputs                   |\n",
    "| JsonOutputParser           | `dict` or Pydantic model | Structured JSON data                    |\n",
    "| YAMLOutputParser           | Pydantic model via YAML  | Human‑friendly structured data          |\n",
    "| XML Parser                 | `dict`                   | XML output parsing                      |\n",
    "| CSV Parser                 | `List[str]`              | CSV‑style lists                         |\n",
    "| EnumOutputParser           | Enum                     | Categorical single‑choice responses     |\n",
    "| DatetimeOutputParser       | `datetime.datetime`      | Date/time extraction                    |\n",
    "| PandasDataFrame Parser     | `pd.DataFrame` or `dict` | Tabular data analysis                   |\n",
    "| StructuredOutputParser     | `Dict[str, str]`         | Lightweight structured responses        |\n",
    "| OutputFixing/Retry Parsers | Wrapped another parser   | Enhanced parsing reliability            |\n",
    "| OpenAIFunctions/Tools      | JSON object              | Use native function‑calling support     |\n",
    "| Custom Parsers             | Any Python type          | Special formats and raw output handling |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔗 Best Practices\n",
    "\n",
    "* Use **native function-call parsers** (OpenAIFunctions/OpenAITools) if available for guaranteed structured output.\n",
    "* Otherwise, choose **JSON/YAML** with Pydantic for robustness.\n",
    "* Use **CSV, Enum, Datetime** for simpler, common data types.\n",
    "* Add **Fixing/Retry wrappers** to handle parsing errors gracefully.\n",
    "* Create **custom parsers** when facing unique output formats.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like example chains showcasing these parsers in action!\n",
    "\n",
    "[1]: https://python.langchain.com/docs/concepts/output_parsers/?utm_source=chatgpt.com \"Output parsers | 🦜️🔗 LangChain\"\n",
    "[2]: https://www.reddit.com/r/LangChain/comments/1fdxh8o?utm_source=chatgpt.com \"PromptTemplate coupled to LLM parsing output\"\n",
    "[3]: https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/custom/?utm_source=chatgpt.com \"Custom Output Parsers | 🦜️🔗 LangChain\"\n",
    "[4]: https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/?utm_source=chatgpt.com \"Output Parsers | 🦜️🔗 LangChain\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955f6ed8",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "Create a simple assistant that uses any LLM and should be pydantic, when we ask about any product it should give you two information product Name, product details tentative price in USD (integer). use chat Prompt Template.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fff6f3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Step 1: Define Pydantic schema for structured output\n",
    "class ProductInfo(BaseModel):\n",
    "    name: str\n",
    "    details: str\n",
    "    price_usd: int\n",
    "\n",
    "# Step 2: Initialize the parser\n",
    "parser = PydanticOutputParser(pydantic_object=ProductInfo)\n",
    "\n",
    "# Step 3: Define the prompt with format instructions\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that provides product information.\"),\n",
    "    (\"human\", \"Give information about the following product: {product_query}\\n{format_instructions}\")\n",
    "])\n",
    "formatted_prompt = prompt.partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "# Step 4: Initialize the ChatGroq model\n",
    "llm = ChatGroq(model=\"gemma2-9b-it\")\n",
    "\n",
    "# Step 5: Chain everything together\n",
    "chain = formatted_prompt | llm | parser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "577cbfff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Product Info ---\n",
      "Name       : HP Victus 16\n",
      "Details    : A gaming laptop featuring an AMD Ryzen processor, NVIDIA GeForce RTX graphics, and a 16.1-inch display.\n",
      "Price (USD): $900\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Ask about a product: \")\n",
    "try:\n",
    "    response = chain.invoke({\"product_query\": query})\n",
    "    print(\"\\n--- Product Info ---\")\n",
    "    print(f\"Name       : {response.name}\")\n",
    "    print(f\"Details    : {response.details}\")\n",
    "    print(f\"Price (USD): ${response.price_usd}\")\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d36d43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
